{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 8640612,
          "sourceType": "datasetVersion",
          "datasetId": 5174487
        }
      ],
      "dockerImageVersionId": 30762,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Library Imports**"
      ],
      "metadata": {
        "id": "lt0xQhY-x9no"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#installations\n",
        "!pip install torchinfo\n",
        "!pip install torchsummary\n",
        "\n",
        "# Basic data manipulations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Handling images\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Handling paths\n",
        "\n",
        "import time\n",
        "\n",
        "# Pytorch essentials\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "\n",
        "# Pytorch essentials for datasets.\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Pytorch way of data augmentation.\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms, utils\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import confusion_matrix , accuracy_score, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "! pip install segmentation-models-pytorch\n",
        "import segmentation_models_pytorch as smp\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T08:07:42.481977Z",
          "iopub.execute_input": "2024-09-23T08:07:42.482384Z",
          "iopub.status.idle": "2024-09-23T08:08:16.431440Z",
          "shell.execute_reply.started": "2024-09-23T08:07:42.482340Z",
          "shell.execute_reply": "2024-09-23T08:08:16.430427Z"
        },
        "trusted": true,
        "id": "D4gnXqMxSfZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Definations**"
      ],
      "metadata": {
        "id": "y2K1r4WiyRVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read img and mask\n",
        "train_img_paths = sorted(glob('train_image_path'))\n",
        "train_mask_paths = sorted(glob('train_mask_path'))\n",
        "train_df = pd.DataFrame({\"images\":train_img_paths,\"masks\":train_mask_paths})\n",
        "train_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T08:08:16.433352Z",
          "iopub.execute_input": "2024-09-23T08:08:16.434202Z",
          "iopub.status.idle": "2024-09-23T08:08:16.462673Z",
          "shell.execute_reply.started": "2024-09-23T08:08:16.434128Z",
          "shell.execute_reply": "2024-09-23T08:08:16.461771Z"
        },
        "trusted": true,
        "id": "3lN5ERylSfZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read img and mask\n",
        "val_img_paths = sorted(glob('valid_image_path'))\n",
        "val_mask_paths = sorted(glob('valid_mask_path'))\n",
        "val_df = pd.DataFrame({\"images\":val_img_paths,\"masks\":val_mask_paths})\n",
        "val_df.head()"
      ],
      "metadata": {
        "id": "q7tlqSx_6MKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read img and mask\n",
        "test_img_paths = sorted(glob('test_image_path'))\n",
        "test_mask_paths = sorted(glob('test_mask_path'))\n",
        "test_df = pd.DataFrame({\"images\":test_img_paths,\"masks\":test_mask_paths})\n",
        "test_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T08:08:20.778434Z",
          "iopub.execute_input": "2024-09-23T08:08:20.779597Z",
          "iopub.status.idle": "2024-09-23T08:08:20.796079Z",
          "shell.execute_reply.started": "2024-09-23T08:08:20.779522Z",
          "shell.execute_reply": "2024-09-23T08:08:20.794959Z"
        },
        "trusted": true,
        "id": "7imsg91-SfZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_imgs = 4\n",
        "idx = np.random.choice(len(train_df), show_imgs, replace=False)\n",
        "fig, axes = plt.subplots(show_imgs*2//4, 4, figsize=(15, 8))\n",
        "axes = axes.flatten()\n",
        "for i, ax in enumerate(axes):\n",
        "    new_i = i//2\n",
        "    if i % 2 ==0 :\n",
        "        full_path = train_df.loc[idx[new_i]]['images']\n",
        "        basename = os.path.basename(full_path)\n",
        "    else:\n",
        "        full_path = train_df.loc[idx[new_i]]['masks']\n",
        "        basename = os.path.basename(full_path) + ' -mask'\n",
        "    ax.imshow(plt.imread(full_path))\n",
        "    ax.set_title(basename)\n",
        "    ax.set_axis_off()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T08:09:48.086773Z",
          "iopub.execute_input": "2024-09-23T08:09:48.087866Z",
          "iopub.status.idle": "2024-09-23T08:09:49.774575Z",
          "shell.execute_reply.started": "2024-09-23T08:09:48.087803Z",
          "shell.execute_reply": "2024-09-23T08:09:49.773577Z"
        },
        "trusted": true,
        "id": "wDL0V1hXSfZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preprocessing**"
      ],
      "metadata": {
        "id": "vgMpxGNl0PUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = A.Compose([\n",
        "    A.Resize(512, 512),\n",
        "    A.RandomCrop(height=512, width=512, always_apply=True),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomRotate90(p=0.5),\n",
        "    A.ShiftScaleRotate(shift_limit=0.01, scale_limit=(-0.04,0.04), rotate_limit=(-15,15), p=0.5),\n",
        "    # A.Normalize(p=1.0),\n",
        "    # ToTensorV2(),\n",
        "])\n",
        "\n",
        "val_transforms = A.Compose([\n",
        "    A.Resize(512, 512),\n",
        "    # ToTensorV2(),\n",
        "])\n",
        "\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataframe, transforms_=None):\n",
        "        self.df = dataframe\n",
        "        # We'll use transforms for data augmentation and converting PIL images to torch tensors.\n",
        "        self.transforms_ = transforms_\n",
        "        self.pre_normalize = v2.Compose([\n",
        "            v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "        self.resize = [512, 512]\n",
        "        self.class_size = 2\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = cv2.cvtColor(cv2.imread(self.df.iloc[index]['images']), cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(self.df.iloc[index]['masks'],cv2.IMREAD_GRAYSCALE)\n",
        "        mask = np.where(mask<127, 0, 1).astype(np.int16)\n",
        "        aug = self.transforms_(image=img, mask=mask)\n",
        "        img, mask = aug['image'], aug['mask']\n",
        "        img = img/255\n",
        "        # img = self.pre_normalize(img)\n",
        "        img = torch.tensor(img, dtype=torch.float).permute(2, 0, 1)\n",
        "        #target = torch.tensor(mask, dtype=torch.long)\n",
        "        # Convert target (mask) to tensor and resize it first\n",
        "        target = torch.tensor(mask, dtype=torch.float)  # Convert to tensor, shape: [1, 512, 512]\n",
        "\n",
        "        # Resize the target tensor before creating the sample dictionary\n",
        "        #target_resized = target.view(3, self.resize[0], self.resize[1])\n",
        "\n",
        "        # Now create the sample dictionary with the resized target\n",
        "        sample = {'x': img, 'y': target}\n",
        "\n",
        "        #sample = {'x': img, 'y': target}\n",
        "        return sample\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T08:10:27.359334Z",
          "iopub.execute_input": "2024-09-23T08:10:27.360138Z",
          "iopub.status.idle": "2024-09-23T08:10:27.375059Z",
          "shell.execute_reply.started": "2024-09-23T08:10:27.360097Z",
          "shell.execute_reply": "2024-09-23T08:10:27.374113Z"
        },
        "trusted": true,
        "id": "t_hbM0sXSfZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n",
        "train_dataset = MyDataset(train_df, train_transforms)\n",
        "val_dataset = MyDataset(val_df, val_transforms)\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "print(f'len train: {len(train_df)}')\n",
        "print(f'len val: {len(val_df)}')\n",
        "print(f'len test: {len(train_df)}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T08:10:29.301910Z",
          "iopub.execute_input": "2024-09-23T08:10:29.302675Z",
          "iopub.status.idle": "2024-09-23T08:10:29.310693Z",
          "shell.execute_reply.started": "2024-09-23T08:10:29.302627Z",
          "shell.execute_reply": "2024-09-23T08:10:29.309235Z"
        },
        "trusted": true,
        "id": "irx_s_jCSfZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attention Modules definition and Activation Function calling**"
      ],
      "metadata": {
        "id": "hbTI7-abSfZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "try:\n",
        "    from inplace_abn import InPlaceABN\n",
        "except ImportError:\n",
        "    InPlaceABN = None\n",
        "\n",
        "\n",
        "class Conv2dReLU(nn.Sequential):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel_size,\n",
        "        padding=0,\n",
        "        stride=1,\n",
        "        use_batchnorm=True,\n",
        "    ):\n",
        "        if use_batchnorm == \"inplace\" and InPlaceABN is None:\n",
        "            raise RuntimeError(\n",
        "                \"In order to use `use_batchnorm='inplace'` inplace_abn package must be installed. \"\n",
        "                + \"To install see: https://github.com/mapillary/inplace_abn\"\n",
        "            )\n",
        "\n",
        "        conv = nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            stride=stride,\n",
        "            padding=padding,\n",
        "            bias=not (use_batchnorm),\n",
        "        )\n",
        "        relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        if use_batchnorm == \"inplace\":\n",
        "            bn = InPlaceABN(out_channels, activation=\"leaky_relu\", activation_param=0.0)\n",
        "            relu = nn.Identity()\n",
        "\n",
        "        elif use_batchnorm and use_batchnorm != \"inplace\":\n",
        "            bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        else:\n",
        "            bn = nn.Identity()\n",
        "\n",
        "        super(Conv2dReLU, self).__init__(conv, bn, relu)\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_channels, in_channels // reduction, bias=False)\n",
        "        self.fc2 = nn.Linear(in_channels // reduction, in_channels, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, h, w = x.size()\n",
        "\n",
        "        # Compute average and max across the spatial dimensions separately\n",
        "        avg_out = torch.mean(x, dim=[2, 3])\n",
        "        max_out, _ = torch.max(torch.max(x, dim=2)[0], dim=2)  # First max over height, then over width\n",
        "\n",
        "        # Apply the fully connected layers to the attention mechanism\n",
        "        out = F.relu(self.fc1(avg_out) + self.fc1(max_out))\n",
        "        out = torch.sigmoid(self.fc2(out)).view(batch_size, channels, 1, 1)\n",
        "        return x * out\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
        "        out = torch.sigmoid(self.conv1(x_cat))\n",
        "        return x * out\n",
        "\n",
        "class GatedAxialAttentionBlock(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        groups=8\n",
        "        kernel_size=7\n",
        "        stride=1\n",
        "        bias=True\n",
        "        # Ensure that in_channels is divisible by groups\n",
        "        if in_channels % groups != 0:\n",
        "            raise ValueError(f\"in_channels ({in_channels}) must be divisible by groups ({groups})\")\n",
        "\n",
        "        # Calculate out_channels for channel attention based on reduction\n",
        "        reduced_channels = in_channels // reduction\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Channel-wise attention (cSE)\n",
        "        self.cSE = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(in_channels, reduced_channels, 1),  # reduce channels for channel attention\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(reduced_channels, in_channels, 1),  # restore channels back to in_channels\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "        # Spatial-wise attention (sSE) with variable kernel size\n",
        "        self.sSE = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 1, kernel_size=kernel_size, padding=kernel_size // 2),  # kernel_size is now adjustable\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Axial attention components with same input/output channels\n",
        "        self.q_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1, groups=groups)\n",
        "        self.k_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1, groups=groups)\n",
        "        self.v_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1, groups=groups)\n",
        "\n",
        "        # Gating mechanism\n",
        "        self.gate = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.size()\n",
        "\n",
        "        # Channel-wise attention (cSE)\n",
        "        channel_att = self.cSE(x)  # Shape: [B, C, 1, 1]\n",
        "        x_c = x * channel_att  # Apply channel-wise attention\n",
        "\n",
        "        # Spatial-wise attention (sSE)\n",
        "        spatial_att = self.sSE(x)  # Shape: [B, 1, H, W]\n",
        "        x_s = x * spatial_att  # Apply spatial-wise attention\n",
        "\n",
        "        # Axial attention mechanism\n",
        "        q = self.q_conv(x)  # Query [B, C, H, W]\n",
        "        k = self.k_conv(x)  # Key [B, C, H, W]\n",
        "        v = self.v_conv(x)  # Value [B, C, H, W]\n",
        "\n",
        "        # Reshape and compute attention map\n",
        "        q_flat = q.view(B, -1, H * W)  # Flatten spatial dimensions\n",
        "        k_flat = k.view(B, -1, H * W)  # Flatten spatial dimensions\n",
        "        v_flat = v.view(B, -1, H * W)  # Flatten spatial dimensions\n",
        "\n",
        "        attn_map = torch.bmm(q_flat.transpose(1, 2), k_flat)  # Dot-product attention\n",
        "        attn_map = torch.softmax(attn_map, dim=-1)  # Apply softmax\n",
        "\n",
        "        out = torch.bmm(v_flat, attn_map.transpose(1, 2))  # Apply attention to value\n",
        "        out = out.view(B, C, H, W)  # Reshape back to original dimensions\n",
        "\n",
        "        # Gating mechanism\n",
        "        gate = self.gate(x)  # Shape: [B, C, H, W]\n",
        "        gated_out = out * gate  # Gated axial attention output\n",
        "\n",
        "        # Combine both channel and spatial attention with gated axial attention output\n",
        "        return x_c + x_s + gated_out\n",
        "\n",
        "class LEQCA_Block(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super(LEQCA_Block, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.reduction = reduction\n",
        "\n",
        "        # Global Average Pooling to get the channel-wise global feature\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        # First fully connected layer for latent entropy approximation\n",
        "        self.fc1 = nn.Conv2d(in_channels, in_channels // reduction, kernel_size=1, bias=False)\n",
        "\n",
        "        # Second fully connected layer to calculate the attention values\n",
        "        self.fc2 = nn.Conv2d(in_channels // reduction, in_channels, kernel_size=1, bias=False)\n",
        "\n",
        "        # Sigmoid to get the attention values between 0 and 1\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, num_channels, h, w = x.size()\n",
        "\n",
        "        # Step 1: Global Average Pooling\n",
        "        avg_out = self.global_pool(x)  # Shape: [batch_size, num_channels, 1, 1]\n",
        "\n",
        "        # Step 2: First fully connected layer with latent entropy\n",
        "        latent_entropy = F.relu(self.fc1(avg_out))  # Shape: [batch_size, num_channels // reduction, 1, 1]\n",
        "\n",
        "        # Step 3: Second fully connected layer to get channel weights\n",
        "        attention_weights = self.fc2(latent_entropy)  # Shape: [batch_size, num_channels, 1, 1]\n",
        "\n",
        "        # Step 4: Apply sigmoid to the attention weights\n",
        "        attention_weights = self.sigmoid(attention_weights)\n",
        "\n",
        "        # Step 5: Multiply input features by attention weights (element-wise)\n",
        "        out = x * attention_weights.expand_as(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ComprehensiveAttention(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16, kernel_size=7):\n",
        "        super(ComprehensiveAttention, self).__init__()\n",
        "        self.channel_attention = ChannelAttention(in_channels, reduction)\n",
        "        self.spatial_attention = SpatialAttention(kernel_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.channel_attention(x)\n",
        "        x = self.spatial_attention(x)\n",
        "        return x\n",
        "\n",
        "class SCSEModule(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super().__init__()\n",
        "        self.cSE = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(in_channels, in_channels // reduction, 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels // reduction, in_channels, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        self.sSE = nn.Sequential(nn.Conv2d(in_channels, 1, 1), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.cSE(x) + x * self.sSE(x)\n",
        "\n",
        "\n",
        "\n",
        "class ArgMax(nn.Module):\n",
        "    def __init__(self, dim=None):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.argmax(x, dim=self.dim)\n",
        "\n",
        "\n",
        "class Clamp(nn.Module):\n",
        "    def __init__(self, min=0, max=1):\n",
        "        super().__init__()\n",
        "        self.min, self.max = min, max\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.clamp(x, self.min, self.max)\n",
        "\n",
        "\n",
        "class Activation(nn.Module):\n",
        "    def __init__(self, name, **params):\n",
        "        super().__init__()\n",
        "\n",
        "        if name is None or name == \"identity\":\n",
        "            self.activation = nn.Identity(**params)\n",
        "        elif name == \"sigmoid\":\n",
        "            self.activation = nn.Sigmoid()\n",
        "        elif name == \"softmax2d\":\n",
        "            self.activation = nn.Softmax(dim=1, **params)\n",
        "        elif name == \"softmax\":\n",
        "            self.activation = nn.Softmax(**params)\n",
        "        elif name == \"logsoftmax\":\n",
        "            self.activation = nn.LogSoftmax(**params)\n",
        "        elif name == \"tanh\":\n",
        "            self.activation = nn.Tanh()\n",
        "        elif name == \"argmax\":\n",
        "            self.activation = ArgMax(**params)\n",
        "        elif name == \"argmax2d\":\n",
        "            self.activation = ArgMax(dim=1, **params)\n",
        "        elif name == \"clamp\":\n",
        "            self.activation = Clamp(**params)\n",
        "        elif callable(name):\n",
        "            self.activation = name(**params)\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Activation should be callable/sigmoid/softmax/logsoftmax/tanh/\"\n",
        "                f\"argmax/argmax2d/clamp/None; got {name}\"\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.activation(x)\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, name, in_channels=None, out_channels=None, **params):\n",
        "        super().__init__()\n",
        "\n",
        "        if name is None:\n",
        "            self.attention = nn.Identity(**params)\n",
        "        elif name == \"scse\":\n",
        "            self.attention = SCSEModule(**params)\n",
        "        elif name == \"comattn\":\n",
        "            self.attention = ComprehensiveAttention(**params)\n",
        "        elif name == \"sa\":\n",
        "            self.attention = SpatialAttention(**params)\n",
        "        elif name == \"ca\":\n",
        "            self.attention = ChannelAttention(**params)\n",
        "        elif name == \"leqca\":\n",
        "            self.attention = LEQCA_Block(**params)\n",
        "        elif name == \"gaa\":\n",
        "            # Explicitly pass in_channels and out_channels\n",
        "            if in_channels is None or out_channels is None:\n",
        "                raise ValueError(\"in_channels and out_channels must be specified for axial attention\")\n",
        "            # Assuming in_channels = 160\n",
        "            # Change reduction to 10 or change groups to a compatible value\n",
        "            # Assuming in_channels = 100\n",
        "            self.attention = GatedAxialAttentionBlock(**params)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Attention {} is not implemented\".format(name))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.attention(x)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T08:13:12.057250Z",
          "iopub.execute_input": "2024-09-23T08:13:12.057633Z",
          "iopub.status.idle": "2024-09-23T08:13:12.088486Z",
          "shell.execute_reply.started": "2024-09-23T08:13:12.057587Z",
          "shell.execute_reply": "2024-09-23T08:13:12.087494Z"
        },
        "trusted": true,
        "id": "4hy4dJRWSfZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "#####from .modules import Activation\n",
        "\n",
        "\n",
        "class SegmentationHead(nn.Sequential):\n",
        "    def __init__(\n",
        "        self, in_channels, out_channels, kernel_size=3, activation=None, upsampling=1\n",
        "    ):\n",
        "        conv2d = nn.Conv2d(\n",
        "            in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2\n",
        "        )\n",
        "        upsampling = (\n",
        "            nn.UpsamplingBilinear2d(scale_factor=upsampling)\n",
        "            if upsampling > 1\n",
        "            else nn.Identity()\n",
        "        )\n",
        "        activation = Activation(activation)\n",
        "        super().__init__(conv2d, upsampling, activation)\n",
        "\n",
        "\n",
        "class ClassificationHead(nn.Sequential):\n",
        "    def __init__(\n",
        "        self, in_channels, classes, pooling=\"avg\", dropout=0.2, activation=None\n",
        "    ):\n",
        "        if pooling not in (\"max\", \"avg\"):\n",
        "            raise ValueError(\n",
        "                \"Pooling should be one of ('max', 'avg'), got {}.\".format(pooling)\n",
        "            )\n",
        "        pool = nn.AdaptiveAvgPool2d(1) if pooling == \"avg\" else nn.AdaptiveMaxPool2d(1)\n",
        "        flatten = nn.Flatten()\n",
        "        dropout = nn.Dropout(p=dropout, inplace=True) if dropout else nn.Identity()\n",
        "        linear = nn.Linear(in_channels, classes, bias=True)\n",
        "        activation = Activation(activation)\n",
        "        super().__init__(pool, flatten, dropout, linear, activation)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T08:13:14.911122Z",
          "iopub.execute_input": "2024-09-23T08:13:14.912037Z",
          "iopub.status.idle": "2024-09-23T08:13:14.921887Z",
          "shell.execute_reply.started": "2024-09-23T08:13:14.911993Z",
          "shell.execute_reply": "2024-09-23T08:13:14.920893Z"
        },
        "trusted": true,
        "id": "JOdZacr8SfZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from segmentation_models_pytorch.base import initialization as init\n",
        "#####from . import initialization as init\n",
        "#####from .hub_mixin import SMPHubMixin\n",
        "from segmentation_models_pytorch.base.hub_mixin import SMPHubMixin\n",
        "\n",
        "class SegmentationModel(torch.nn.Module, SMPHubMixin):\n",
        "    def initialize(self):\n",
        "        init.initialize_decoder(self.decoder)\n",
        "        init.initialize_head(self.segmentation_head)\n",
        "        if self.classification_head is not None:\n",
        "            init.initialize_head(self.classification_head)\n",
        "\n",
        "    def check_input_shape(self, x):\n",
        "        h, w = x.shape[-2:]\n",
        "        output_stride = self.encoder.output_stride\n",
        "        if h % output_stride != 0 or w % output_stride != 0:\n",
        "            new_h = (\n",
        "                (h // output_stride + 1) * output_stride\n",
        "                if h % output_stride != 0\n",
        "                else h\n",
        "            )\n",
        "            new_w = (\n",
        "                (w // output_stride + 1) * output_stride\n",
        "                if w % output_stride != 0\n",
        "                else w\n",
        "            )\n",
        "            raise RuntimeError(\n",
        "                f\"Wrong input shape height={h}, width={w}. Expected image height and width \"\n",
        "                f\"divisible by {output_stride}. Consider pad your images to shape ({new_h}, {new_w}).\"\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\n",
        "\n",
        "        self.check_input_shape(x)\n",
        "\n",
        "        features = self.encoder(x)\n",
        "        decoder_output = self.decoder(*features)\n",
        "\n",
        "        masks = self.segmentation_head(decoder_output)\n",
        "\n",
        "        if self.classification_head is not None:\n",
        "            labels = self.classification_head(features[-1])\n",
        "            return masks, labels\n",
        "\n",
        "        return masks\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, x):\n",
        "        \"\"\"Inference method. Switch model to `eval` mode, call `.forward(x)` with `torch.no_grad()`\n",
        "\n",
        "        Args:\n",
        "            x: 4D torch tensor with shape (batch_size, channels, height, width)\n",
        "\n",
        "        Return:\n",
        "            prediction: 4D torch tensor with shape (batch_size, classes, height, width)\n",
        "\n",
        "        \"\"\"\n",
        "        if self.training:\n",
        "            self.eval()\n",
        "\n",
        "        x = self.forward(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T08:13:17.865100Z",
          "iopub.execute_input": "2024-09-23T08:13:17.865835Z",
          "iopub.status.idle": "2024-09-23T08:13:17.877050Z",
          "shell.execute_reply.started": "2024-09-23T08:13:17.865794Z",
          "shell.execute_reply": "2024-09-23T08:13:17.876182Z"
        },
        "trusted": true,
        "id": "ZPlA35goSfZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoder Block Definition and Unet Decoder**"
      ],
      "metadata": {
        "id": "R4eM3YDfSfZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#####from segmentation_models_pytorch.base import modules as md\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        skip_channels,\n",
        "        out_channels,\n",
        "        use_batchnorm=True,\n",
        "        attention_type=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.conv1 = Conv2dReLU(\n",
        "            in_channels + skip_channels,\n",
        "            out_channels,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            use_batchnorm=use_batchnorm,\n",
        "        )\n",
        "        self.attention1 = Attention(\n",
        "            attention_type, in_channels=in_channels + skip_channels\n",
        "        )\n",
        "        self.conv2 = Conv2dReLU(\n",
        "            out_channels,\n",
        "            out_channels,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            use_batchnorm=use_batchnorm,\n",
        "        )\n",
        "        self.attention2 = Attention(attention_type, in_channels=out_channels)\n",
        "\n",
        "    def forward(self, x, skip=None):\n",
        "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
        "        if skip is not None:\n",
        "            x = torch.cat([x, skip], dim=1)\n",
        "            x = self.attention1(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.attention2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CenterBlock(nn.Sequential):\n",
        "    def __init__(self, in_channels, out_channels, use_batchnorm=True):\n",
        "        conv1 = Conv2dReLU(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            use_batchnorm=use_batchnorm,\n",
        "        )\n",
        "        conv2 = Conv2dReLU(\n",
        "            out_channels,\n",
        "            out_channels,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            use_batchnorm=use_batchnorm,\n",
        "        )\n",
        "        super().__init__(conv1, conv2)\n",
        "\n",
        "\n",
        "class UnetDecoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder_channels,\n",
        "        decoder_channels,\n",
        "        n_blocks=5,\n",
        "        use_batchnorm=True,\n",
        "        attention_type=None,\n",
        "        center=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        if n_blocks != len(decoder_channels):\n",
        "            raise ValueError(\n",
        "                \"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n",
        "                    n_blocks, len(decoder_channels)\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # remove first skip with same spatial resolution\n",
        "        encoder_channels = encoder_channels[1:]\n",
        "        # reverse channels to start from head of encoder\n",
        "        encoder_channels = encoder_channels[::-1]\n",
        "\n",
        "        # computing blocks input and output channels\n",
        "        head_channels = encoder_channels[0]\n",
        "        in_channels = [head_channels] + list(decoder_channels[:-1])\n",
        "        skip_channels = list(encoder_channels[1:]) + [0]\n",
        "        out_channels = decoder_channels\n",
        "\n",
        "        if center:\n",
        "            self.center = CenterBlock(\n",
        "                head_channels, head_channels, use_batchnorm=use_batchnorm\n",
        "            )\n",
        "        else:\n",
        "            self.center = nn.Identity()\n",
        "\n",
        "        # combine decoder keyword arguments\n",
        "        kwargs = dict(use_batchnorm=use_batchnorm, attention_type=attention_type)\n",
        "        blocks = [\n",
        "            DecoderBlock(in_ch, skip_ch, out_ch, **kwargs)\n",
        "            for in_ch, skip_ch, out_ch in zip(in_channels, skip_channels, out_channels)\n",
        "        ]\n",
        "        self.blocks = nn.ModuleList(blocks)\n",
        "\n",
        "    def forward(self, *features):\n",
        "        features = features[1:]  # remove first skip with same spatial resolution\n",
        "        features = features[::-1]  # reverse channels to start from head of encoder\n",
        "\n",
        "        head = features[0]\n",
        "        skips = features[1:]\n",
        "\n",
        "        x = self.center(head)\n",
        "        for i, decoder_block in enumerate(self.blocks):\n",
        "            skip = skips[i] if i < len(skips) else None\n",
        "            x = decoder_block(x, skip)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T08:13:19.193188Z",
          "iopub.execute_input": "2024-09-23T08:13:19.194048Z",
          "iopub.status.idle": "2024-09-23T08:13:19.212692Z",
          "shell.execute_reply.started": "2024-09-23T08:13:19.194007Z",
          "shell.execute_reply": "2024-09-23T08:13:19.211653Z"
        },
        "trusted": true,
        "id": "xgSkE62uSfZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoder Block with fused attention and Inceptionv4 backbone**"
      ],
      "metadata": {
        "id": "0qmn6iUGSYfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import functools\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "from pretrainedmodels.models.inceptionv4 import InceptionV4, pretrained_settings\n",
        "\n",
        "\n",
        "# ECA Block (Efficient Channel Attention)\n",
        "class ECA(nn.Module):\n",
        "    def __init__(self, in_channels, kernel_size=3):\n",
        "        super(ECA, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        # Ensure kernel size is odd\n",
        "        if self.kernel_size % 2 == 0:\n",
        "            self.kernel_size += 1\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.conv = nn.Conv1d(in_channels, in_channels, kernel_size=self.kernel_size, padding=self.kernel_size // 2, groups=in_channels, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, height, width = x.size()\n",
        "\n",
        "        # Squeeze operation\n",
        "        y = self.avg_pool(x).view(batch_size, channels)\n",
        "\n",
        "        # Channel-wise convolution\n",
        "        y = y.unsqueeze(2)  # Add a dimension for Conv1d\n",
        "        y = self.conv(y)\n",
        "        y = y.view(batch_size, channels, 1)\n",
        "\n",
        "        # Apply sigmoid activation\n",
        "        y = self.sigmoid(y)\n",
        "\n",
        "        # Scale the input\n",
        "        return x * y.view(batch_size, channels, 1, 1)\n",
        "\n",
        "\n",
        "\n",
        "# LEQCA Block (Local Equivariant and Quality Channel Attention)\n",
        "class LEQCA(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(LEQCA, self).__init__()\n",
        "        self.local_pool = nn.AdaptiveAvgPool2d(2)  # Keep local pooling\n",
        "        self.fc1 = nn.Conv2d(in_channels, in_channels // 16, kernel_size=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc2 = nn.Conv2d(in_channels // 16, in_channels, kernel_size=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        scale = self.local_pool(x)\n",
        "        scale = self.fc1(scale)\n",
        "        scale = self.relu(scale)\n",
        "        scale = self.fc2(scale)\n",
        "        scale = self.sigmoid(scale)\n",
        "        scale = F.interpolate(scale, size=x.shape[2:], mode='bilinear', align_corners=False)  # Upsample to match input\n",
        "        return x * scale\n",
        "\n",
        "\n",
        "\n",
        "# Combined Ensemble Attention Block (ECA + LEQCA)\n",
        "class EnsembleAttention(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(EnsembleAttention, self).__init__()\n",
        "        self.eca_block = ECA(in_channels)\n",
        "        self.leqca_block = LEQCA(in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        eca_out = self.eca_block(x)\n",
        "        leqca_out = self.leqca_block(x)\n",
        "        ensemble_out = (eca_out + leqca_out) / 2  # Averaging outputs\n",
        "        return ensemble_out\n",
        "\n",
        "\n",
        "\n",
        "# Encoder Mixin\n",
        "class EncoderMixin:\n",
        "    _output_stride = 32\n",
        "\n",
        "    @property\n",
        "    def out_channels(self):\n",
        "        return self._out_channels[:self._depth + 1]\n",
        "\n",
        "    @property\n",
        "    def output_stride(self):\n",
        "        return min(self._output_stride, 2 ** self._depth)\n",
        "\n",
        "    def set_in_channels(self, in_channels, pretrained=True):\n",
        "        if in_channels == 3:\n",
        "            return\n",
        "        self._in_channels = in_channels\n",
        "        if self._out_channels[0] == 3:\n",
        "            self._out_channels = tuple([in_channels] + list(self._out_channels)[1:])\n",
        "        patch_first_conv(self, new_in_channels=in_channels, pretrained=pretrained)\n",
        "\n",
        "    def make_dilated(self, output_stride):\n",
        "        if output_stride == 16:\n",
        "            stage_list, dilation_list = [5], [2]\n",
        "        elif output_stride == 8:\n",
        "            stage_list, dilation_list = [4, 5], [2, 4]\n",
        "        else:\n",
        "            raise ValueError(\"Output stride must be 16 or 8.\")\n",
        "        self._output_stride = output_stride\n",
        "        stages = self.get_stages()\n",
        "        for stage_idx, dilation_rate in zip(stage_list, dilation_list):\n",
        "            replace_strides_with_dilation(stages[stage_idx], dilation_rate)\n",
        "\n",
        "\n",
        "# InceptionV4 Encoder with MSCA + LEQCA Ensemble Attention\n",
        "class InceptionV4Encoder(InceptionV4, EncoderMixin):\n",
        "    def __init__(self, stage_idxs, out_channels, depth=5, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self._stage_idxs = stage_idxs\n",
        "        self._out_channels = out_channels\n",
        "        self._depth = depth\n",
        "        self._in_channels = 3\n",
        "        self.attention_blocks = nn.ModuleList(\n",
        "            [EnsembleAttention(out_channels[i]) for i in range(1, depth + 1)]\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) and m.kernel_size == (3, 3):\n",
        "                m.padding = (1, 1)\n",
        "            if isinstance(m, nn.MaxPool2d):\n",
        "                m.padding = (1, 1)\n",
        "        del self.last_linear\n",
        "\n",
        "    def get_stages(self):\n",
        "        \"\"\"Defines the stages based on the stage indexes provided.\"\"\"\n",
        "        return [\n",
        "            nn.Identity(),  # First stage is a placeholder (no processing)\n",
        "            self.features[:self._stage_idxs[0]],  # First block of layers\n",
        "            self.features[self._stage_idxs[0]:self._stage_idxs[1]],  # Second block of layers\n",
        "            self.features[self._stage_idxs[1]:self._stage_idxs[2]],  # Third block of layers\n",
        "            self.features[self._stage_idxs[2]:self._stage_idxs[3]],  # Fourth block of layers\n",
        "            self.features[self._stage_idxs[3]:],  # Fifth block of layers\n",
        "        ]\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        for i in range(self._depth + 1):\n",
        "            x = self.get_stages()[i](x)\n",
        "            if i > 0:\n",
        "                x = self.attention_blocks[i - 1](x)  # Apply ensemble attention (MSCA + LEQCA)\n",
        "            features.append(x)\n",
        "        return features\n",
        "\n",
        "    def load_state_dict(self, state_dict, **kwargs):\n",
        "        state_dict.pop(\"last_linear.bias\", None)\n",
        "        state_dict.pop(\"last_linear.weight\", None)\n",
        "        super().load_state_dict(state_dict, **kwargs)\n",
        "\n",
        "\n",
        "# Utility Functions\n",
        "def patch_first_conv(model, new_in_channels, default_in_channels=3, pretrained=True):\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, nn.Conv2d) and module.in_channels == default_in_channels:\n",
        "            break\n",
        "    weight = module.weight.detach()\n",
        "    module.in_channels = new_in_channels\n",
        "    if not pretrained:\n",
        "        module.weight = nn.parameter.Parameter(\n",
        "            torch.Tensor(module.out_channels, new_in_channels // module.groups, *module.kernel_size))\n",
        "        module.reset_parameters()\n",
        "    elif new_in_channels == 1:\n",
        "        module.weight = nn.parameter.Parameter(weight.sum(1, keepdim=True))\n",
        "    else:\n",
        "        new_weight = torch.Tensor(module.out_channels, new_in_channels // module.groups, *module.kernel_size)\n",
        "        for i in range(new_in_channels):\n",
        "            new_weight[:, i] = weight[:, i % default_in_channels]\n",
        "        new_weight *= (default_in_channels / new_in_channels)\n",
        "        module.weight = nn.parameter.Parameter(new_weight)\n",
        "\n",
        "\n",
        "def replace_strides_with_dilation(module, dilation_rate):\n",
        "    for mod in module.modules():\n",
        "        if isinstance(mod, nn.Conv2d):\n",
        "            mod.stride = (1, 1)\n",
        "            mod.dilation = (dilation_rate, dilation_rate)\n",
        "            kh, kw = mod.kernel_size\n",
        "            mod.padding = ((kh // 2) * dilation_rate, (kw // 2) * dilation_rate)\n",
        "            if hasattr(mod, \"static_padding\"):\n",
        "                mod.static_padding = nn.Identity()\n",
        "\n",
        "\n",
        "# Preprocessing Functions\n",
        "def preprocess_input(x, mean=None, std=None, input_space=\"RGB\", input_range=None, **kwargs):\n",
        "    if input_space == \"BGR\":\n",
        "        x = x[..., ::-1].copy()\n",
        "    if input_range is not None and x.max() > 1 and input_range[1] == 1:\n",
        "        x = x / 255.0\n",
        "    if mean is not None:\n",
        "        x -= np.array(mean)\n",
        "    if std is not None:\n",
        "        x /= np.array(std)\n",
        "    return x\n",
        "\n",
        "\n",
        "# Encoder Retrieval Functions\n",
        "inceptionv4_encoders = {\n",
        "    \"inceptionv4\": {\n",
        "        \"encoder\": InceptionV4Encoder,\n",
        "        \"pretrained_settings\": pretrained_settings[\"inceptionv4\"],\n",
        "        \"params\": {\n",
        "            \"stage_idxs\": (3, 5, 9, 15),\n",
        "            \"out_channels\": (3, 64, 192, 384, 1024, 1536),\n",
        "            \"num_classes\": 1001,\n",
        "        },\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "def get_encoder(name, in_channels=3, depth=5, weights=None, output_stride=32, **kwargs):\n",
        "    if name not in inceptionv4_encoders:\n",
        "        raise KeyError(f\"Unsupported encoder `{name}`, supported: {list(inceptionv4_encoders.keys())}\")\n",
        "\n",
        "    Encoder = inceptionv4_encoders[name][\"encoder\"]\n",
        "    params = inceptionv4_encoders[name][\"params\"]\n",
        "    params.update(depth=depth)\n",
        "    encoder = Encoder(**params)\n",
        "\n",
        "    if weights:\n",
        "        settings = inceptionv4_encoders[name][\"pretrained_settings\"][weights]\n",
        "        pretrained_dict = model_zoo.load_url(settings[\"url\"])\n",
        "        model_dict = encoder.state_dict()\n",
        "\n",
        "        # Filter out attention block weights, which are not present in pre-trained weights\n",
        "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and \"attention_blocks\" not in k}\n",
        "        model_dict.update(pretrained_dict)\n",
        "\n",
        "        encoder.load_state_dict(model_dict)\n",
        "\n",
        "    encoder.set_in_channels(in_channels, pretrained=weights is not None)\n",
        "    if output_stride != 32:\n",
        "        encoder.make_dilated(output_stride=output_stride)\n",
        "\n",
        "    return encoder\n",
        "\n",
        "\n",
        "# Preprocessing functions\n",
        "def get_preprocessing_params(encoder_name, pretrained=\"imagenet\"):\n",
        "    settings = inceptionv4_encoders[encoder_name][\"pretrained_settings\"].get(pretrained)\n",
        "    return {\n",
        "        \"input_space\": settings.get(\"input_space\", \"RGB\"),\n",
        "        \"input_range\": list(settings.get(\"input_range\", [0, 1])),\n",
        "        \"mean\": list(settings[\"mean\"]),\n",
        "        \"std\": list(settings[\"std\"]),\n",
        "    }\n",
        "\n",
        "\n",
        "def get_preprocessing_fn(encoder_name, pretrained=\"imagenet\"):\n",
        "    params = get_preprocessing_params(encoder_name, pretrained)\n",
        "    return functools.partial(preprocess_input, **params)\n"
      ],
      "metadata": {
        "id": "V45T5tkqhnSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, Union, List\n",
        "\"\"\"\n",
        "from segmentation_models_pytorch.encoders import get_encoder\n",
        "from segmentation_models_pytorch.base import (\n",
        "    SegmentationModel,\n",
        "    SegmentationHead,\n",
        "    ClassificationHead,\n",
        ")\n",
        "\"\"\"\n",
        "#####from .decoder import UnetDecoder\n",
        "\n",
        "class Unet(SegmentationModel):\n",
        "    \"\"\"Unet_ is a fully convolution neural network for image semantic segmentation. Consist of *encoder*\n",
        "    and *decoder* parts connected with *skip connections*. Encoder extract features of different spatial\n",
        "    resolution (skip connections) which are used by decoder to define accurate segmentation mask. Use *concatenation*\n",
        "    for fusing decoder blocks with skip connections.\n",
        "\n",
        "    Args:\n",
        "        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n",
        "            to extract features of different spatial resolution\n",
        "        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features\n",
        "            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n",
        "            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n",
        "            Default is 5\n",
        "        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and\n",
        "            other pretrained weights (see table with available weights for each encoder_name)\n",
        "        decoder_channels: List of integers which specify **in_channels** parameter for convolutions used in decoder.\n",
        "            Length of the list should be the same as **encoder_depth**\n",
        "        decoder_use_batchnorm: If **True**, BatchNorm2d layer between Conv2D and Activation layers\n",
        "            is used. If **\"inplace\"** InplaceABN will be used, allows to decrease memory consumption.\n",
        "            Available options are **True, False, \"inplace\"**\n",
        "        decoder_attention_type: Attention module used in decoder of the model. Available options are\n",
        "            **None** and **scse** (https://arxiv.org/abs/1808.08127).\n",
        "        in_channels: A number of input channels for the model, default is 3 (RGB images)\n",
        "        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n",
        "        activation: An activation function to apply after the final convolution layer.\n",
        "            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**,\n",
        "                **callable** and **None**.\n",
        "            Default is **None**\n",
        "        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build\n",
        "            on top of encoder if **aux_params** is not **None** (default). Supported params:\n",
        "                - classes (int): A number of classes\n",
        "                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n",
        "                - dropout (float): Dropout factor in [0, 1)\n",
        "                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\"\n",
        "                    (could be **None** to return logits)\n",
        "\n",
        "    Returns:\n",
        "        ``torch.nn.Module``: Unet\n",
        "\n",
        "    .. _Unet:\n",
        "        https://arxiv.org/abs/1505.04597\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder_name: str = \"resnet34\",\n",
        "        encoder_depth: int = 5,\n",
        "        encoder_weights: Optional[str] = \"imagenet\",\n",
        "        decoder_use_batchnorm: bool = True,\n",
        "        decoder_channels: List[int] = (256, 128, 64, 32, 16),\n",
        "        decoder_attention_type: Optional[str] = None,\n",
        "        in_channels: int = 3,\n",
        "        classes: int = 1,\n",
        "        activation: Optional[Union[str, callable]] = None,\n",
        "        aux_params: Optional[dict] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = get_encoder(\n",
        "            encoder_name,\n",
        "            in_channels=in_channels,\n",
        "            depth=encoder_depth,\n",
        "            weights=encoder_weights,\n",
        "        )\n",
        "\n",
        "        self.decoder = UnetDecoder(\n",
        "            encoder_channels=self.encoder.out_channels,\n",
        "            decoder_channels=decoder_channels,\n",
        "            n_blocks=encoder_depth,\n",
        "            use_batchnorm=decoder_use_batchnorm,\n",
        "            center=True if encoder_name.startswith(\"vgg\") else False,\n",
        "            attention_type=decoder_attention_type,\n",
        "        )\n",
        "\n",
        "        self.segmentation_head = SegmentationHead(\n",
        "            in_channels=decoder_channels[-1],\n",
        "            out_channels=classes,\n",
        "            activation=activation,\n",
        "            kernel_size=3,\n",
        "        )\n",
        "\n",
        "        if aux_params is not None:\n",
        "            self.classification_head = ClassificationHead(\n",
        "                in_channels=self.encoder.out_channels[-1], **aux_params\n",
        "            )\n",
        "        else:\n",
        "            self.classification_head = None\n",
        "\n",
        "        self.name = \"u-{}\".format(encoder_name)\n",
        "        self.initialize()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T08:13:21.539740Z",
          "iopub.execute_input": "2024-09-23T08:13:21.540504Z",
          "iopub.status.idle": "2024-09-23T08:13:21.554321Z",
          "shell.execute_reply.started": "2024-09-23T08:13:21.540460Z",
          "shell.execute_reply": "2024-09-23T08:13:21.553346Z"
        },
        "trusted": true,
        "id": "X1xigd_DSfZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "PLjl-NiI4XWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchinfo import summary\n",
        "#from model import Unet  # Assuming your model is saved in model.py\n",
        "\n",
        "# Define the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class_size = 1\n",
        "model = Unet(\n",
        "    encoder_name=\"inceptionv4\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
        "    encoder_weights=\"imagenet\",             # use `imagenet` pre-trained weights for encoder initialization\n",
        "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
        "    classes=class_size                      # model output channels (number of classes in your dataset)\n",
        ").to(device)\n",
        "\n",
        "# Get the summary\n",
        "input_size = (1, 3, 512, 512)  # Batch size of 1, 3 channels, 512x512 image\n",
        "summary(model, input_size=input_size, device=device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T08:14:40.801077Z",
          "iopub.execute_input": "2024-09-23T08:14:40.801469Z",
          "iopub.status.idle": "2024-09-23T08:14:41.119350Z",
          "shell.execute_reply.started": "2024-09-23T08:14:40.801434Z",
          "shell.execute_reply": "2024-09-23T08:14:41.118439Z"
        },
        "trusted": true,
        "id": "uswZCOReSfZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dice score implementation\n",
        "def dice_score(pred, target, smooth=1e-6):\n",
        "    pred = pred > 0.5  # Threshold predictions\n",
        "    target = target > 0.5\n",
        "    intersection = (pred * target).sum().float()\n",
        "    dice = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
        "    return dice\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, lr_scheduler):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_iou_score = 0\n",
        "    epoch_dice_score = 0\n",
        "\n",
        "    for batch_i, batch in enumerate(dataloader):\n",
        "        x, y = batch['x'].to(device), batch['y'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        pred = torch.sigmoid(pred)\n",
        "        pred = pred.squeeze(dim=1)\n",
        "        y = y.round().long()\n",
        "\n",
        "        # Calculate Dice score\n",
        "        dice = dice_score(pred, y)\n",
        "        epoch_dice_score += dice.item()\n",
        "\n",
        "        # Calculate IoU score\n",
        "        tp, fp, fn, tn = smp.metrics.get_stats(pred, y, mode='binary', threshold=0.5)\n",
        "        iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\").item()\n",
        "        epoch_iou_score += iou\n",
        "\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    return epoch_loss / num_batches, epoch_dice_score / num_batches, epoch_iou_score / num_batches\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_iou_score = 0\n",
        "    epoch_dice_score = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_i, batch in enumerate(dataloader):\n",
        "            x, y = batch['x'].to(device), batch['y'].to(device)\n",
        "\n",
        "            pred = model(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            pred = torch.sigmoid(pred)\n",
        "            pred = pred.squeeze(dim=1)\n",
        "            y = y.round().long()\n",
        "\n",
        "            # Calculate Dice score\n",
        "            dice = dice_score(pred, y)\n",
        "            epoch_dice_score += dice.item()\n",
        "\n",
        "            # Calculate IoU score\n",
        "            tp, fp, fn, tn = smp.metrics.get_stats(pred, y, mode='binary', threshold=0.5)\n",
        "            iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\").item()\n",
        "            epoch_iou_score += iou\n",
        "\n",
        "    return epoch_loss / num_batches, epoch_dice_score / num_batches, epoch_iou_score / num_batches\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-22T20:08:32.384838Z",
          "iopub.execute_input": "2024-09-22T20:08:32.385308Z",
          "iopub.status.idle": "2024-09-22T20:08:32.404177Z",
          "shell.execute_reply.started": "2024-09-22T20:08:32.385261Z",
          "shell.execute_reply": "2024-09-22T20:08:32.402917Z"
        },
        "trusted": true,
        "id": "PWImhsI-SfZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "logs = {\n",
        "    'train_loss': [], 'val_loss': [],\n",
        "    'train_iou_score': [], 'val_iou_score': [],\n",
        "    'train_dice_score': [], 'val_dice_score': []\n",
        "}\n",
        "\n",
        "\n",
        "if os.path.exists('checkpoints') == False:\n",
        "    os.mkdir(\"checkpoints\")\n",
        "\n",
        "loss_fn = smp.losses.DiceLoss(mode=\"binary\")\n",
        "#loss_fn = smp.losses.FocalLoss(mode=\"binary\")\n",
        "\n",
        "#loss_fn = DiceLoss(mode=\"binary\")\n",
        "#loss_fn = FocalLoss(mode=\"binary\")\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size = 100, gamma=0.1)\n",
        "\n",
        "# Earlystopping\n",
        "patience = 5\n",
        "counter = 0\n",
        "best_loss = np.inf\n",
        "\n",
        "model.to(device)\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "    train_loss, train_dice_score, train_iou_score = train(train_loader, model, loss_fn, optimizer, step_lr_scheduler)\n",
        "    val_loss, val_dice_score, val_iou_score = test(val_loader, model, loss_fn)\n",
        "\n",
        "    logs['train_loss'].append(train_loss)\n",
        "    logs['val_loss'].append(val_loss)\n",
        "    logs['train_dice_score'].append(train_dice_score)\n",
        "    logs['val_dice_score'].append(val_dice_score)\n",
        "    logs['train_iou_score'].append(train_iou_score)\n",
        "    logs['val_iou_score'].append(val_iou_score)\n",
        "\n",
        "    print(f'EPOCH: {str(epoch+1).zfill(3)} | '\n",
        "          f'train_loss: {train_loss:.4f}, val_loss: {val_loss:.4f} | '\n",
        "          f'train_dice_score: {train_dice_score:.3f}, val_dice_score: {val_dice_score:.3f} | '\n",
        "          f'train_iou_score: {train_iou_score:.3f}, val_iou_score: {val_iou_score:.3f} | '\n",
        "          f'lr: {optimizer.param_groups[0][\"lr\"]}')\n",
        "\n",
        "    # Save model\n",
        "    torch.save(model.state_dict(), \"checkpoints/last.pth\")\n",
        "    if val_loss < best_loss:\n",
        "        counter = 0\n",
        "        best_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"checkpoints/best.pth\")\n",
        "    else:\n",
        "        counter += 1\n",
        "\n",
        "    # Early stopping\n",
        "    if counter >= patience:\n",
        "        print(\"Early stopping!\")\n",
        "        break\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-22T20:08:42.218186Z",
          "iopub.execute_input": "2024-09-22T20:08:42.218886Z",
          "iopub.status.idle": "2024-09-22T20:22:16.577140Z",
          "shell.execute_reply.started": "2024-09-22T20:08:42.218845Z",
          "shell.execute_reply": "2024-09-22T20:22:16.576163Z"
        },
        "trusted": true,
        "id": "esjwd17BSfZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(logs['train_loss'],label='Train_Loss')\n",
        "plt.plot(logs['val_loss'],label='Validation_Loss')\n",
        "plt.title('Train_Loss & Validation_Loss',fontsize=20)\n",
        "plt.legend()\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(logs['train_iou_score'],label='Train_Iou_Score')\n",
        "plt.plot(logs['val_iou_score'],label='Validation_Iou_Score')\n",
        "plt.title('Train_Iou_score & Validation_Iou_score',fontsize=20)\n",
        "plt.legend()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-22T20:22:16.578828Z",
          "iopub.execute_input": "2024-09-22T20:22:16.579138Z",
          "iopub.status.idle": "2024-09-22T20:22:17.341972Z",
          "shell.execute_reply.started": "2024-09-22T20:22:16.579106Z",
          "shell.execute_reply": "2024-09-22T20:22:17.340915Z"
        },
        "trusted": true,
        "id": "Va7TA_B-SfZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing**"
      ],
      "metadata": {
        "id": "4PKSOdnU4hCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_transforms = A.Compose([\n",
        "    A.Resize(512, 512),\n",
        "    # ToTensorV2(),\n",
        "])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-22T20:22:17.343459Z",
          "iopub.execute_input": "2024-09-22T20:22:17.343939Z",
          "iopub.status.idle": "2024-09-22T20:22:17.349783Z",
          "shell.execute_reply.started": "2024-09-22T20:22:17.343887Z",
          "shell.execute_reply": "2024-09-22T20:22:17.348768Z"
        },
        "trusted": true,
        "id": "bzC_fnt9SfZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataframe,transforms_=None):\n",
        "        self.df = dataframe\n",
        "        self.transforms_ = transforms_\n",
        "        self.pre_normalize = v2.Compose([\n",
        "            v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "        self.resize = [512, 512]\n",
        "        self.class_size = 2\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = cv2.cvtColor(cv2.imread(self.df.iloc[index]['images']), cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(self.df.iloc[index]['masks'],cv2.IMREAD_GRAYSCALE)\n",
        "        aug = self.transforms_(image=img, mask=mask)\n",
        "        img, mask = aug['image'], aug['mask']\n",
        "        img_view = np.copy(img)\n",
        "        img = img/255\n",
        "        # img = self.pre_normalize(img)\n",
        "        img = torch.tensor(img, dtype=torch.float).permute(2, 0, 1)\n",
        "        mask_view = np.copy(mask)\n",
        "        mask = np.where(mask<127, 0, 1).astype(np.int16)\n",
        "        target = torch.tensor(mask, dtype=torch.long)\n",
        "        sample = {'x': img, 'y': target, 'img_view':img_view, 'mask_view':mask_view}\n",
        "        return sample\n",
        "\n",
        "test_dataset = TestDataset(test_df, test_transforms)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-22T20:22:17.351914Z",
          "iopub.execute_input": "2024-09-22T20:22:17.352258Z",
          "iopub.status.idle": "2024-09-22T20:22:17.368203Z",
          "shell.execute_reply.started": "2024-09-22T20:22:17.352221Z",
          "shell.execute_reply": "2024-09-22T20:22:17.367357Z"
        },
        "trusted": true,
        "id": "9FKNTXrfSfZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('checkpoints/best.pth'))\n",
        "model.to(device)\n",
        "\n",
        "def get_metrics(model, dataloader, threshold):\n",
        "    IoU_score, precision, f1_score, recall, acc, dice_score = 0, 0, 0, 0, 0, 0\n",
        "    batches = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_i, batch in enumerate(dataloader):\n",
        "            x, y = batch['x'].to(device), batch['y'].to(device)  # move data to GPU\n",
        "            pred = model(x)\n",
        "            pred = pred.squeeze(dim=1)\n",
        "            pred = torch.sigmoid(pred)\n",
        "            y = y.round().long()\n",
        "\n",
        "            # Calculate stats\n",
        "            tp, fp, fn, tn = smp.metrics.get_stats(pred, y, mode='binary', threshold=threshold)\n",
        "\n",
        "            # Calculate various metrics\n",
        "            batch_iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\").item()\n",
        "            batch_acc = smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"micro\").item()\n",
        "            batch_f1_score = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro\").item()\n",
        "            batch_recall = smp.metrics.recall(tp, fp, fn, tn, reduction=\"micro\").item()\n",
        "            batch_precision = smp.metrics.precision(tp, fp, fn, tn, reduction=\"micro\").item()\n",
        "\n",
        "            # Manually calculate Dice score\n",
        "            dice = (2 * tp.sum()) / (2 * tp.sum() + fp.sum() + fn.sum())\n",
        "            batch_dice_score = dice.item()\n",
        "\n",
        "            # Aggregate the results\n",
        "            IoU_score += batch_iou_score\n",
        "            acc += batch_acc\n",
        "            f1_score += batch_f1_score\n",
        "            recall += batch_recall\n",
        "            precision += batch_precision\n",
        "            dice_score += batch_dice_score\n",
        "            batches += 1\n",
        "\n",
        "    # Compute average metrics over all batches\n",
        "    IoU_score = round(IoU_score / batches, 3)\n",
        "    precision = round(precision / batches, 3)\n",
        "    f1_score = round(f1_score / batches, 3)\n",
        "    recall = round(recall / batches, 3)\n",
        "    acc = round(acc / batches, 3)\n",
        "    dice_score = round(dice_score / batches, 3)\n",
        "\n",
        "    sample = {\n",
        "        'iou': IoU_score,\n",
        "        'pre': precision,\n",
        "        'fi': f1_score,\n",
        "        're': recall,\n",
        "        'acc': acc,\n",
        "        'dice': dice_score\n",
        "    }\n",
        "    return sample\n",
        "\n",
        "# Evaluate the model for different thresholds\n",
        "threshold_list = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
        "for threshold in threshold_list:\n",
        "    sample = get_metrics(model, test_loader, threshold)\n",
        "    print(f\"Threshold: {threshold:.2f} \\\n",
        "    IoU Score: {sample['iou']:.3f} \\\n",
        "    Precision: {sample['pre']:.3f} \\\n",
        "    F1 Score: {sample['fi']:.3f} \\\n",
        "    Recall: {sample['re']:.3f} \\\n",
        "    Accuracy: {sample['acc']:.3f} \\\n",
        "    Dice Score: {sample['dice']:.3f}\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-22T20:22:17.369633Z",
          "iopub.execute_input": "2024-09-22T20:22:17.370046Z",
          "iopub.status.idle": "2024-09-22T20:24:04.443818Z",
          "shell.execute_reply.started": "2024-09-22T20:22:17.369996Z",
          "shell.execute_reply": "2024-09-22T20:24:04.442541Z"
        },
        "trusted": true,
        "id": "gfJgVl_wSfZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot\n",
        "model.load_state_dict(torch.load(\"checkpoints/best.pth\"))\n",
        "model.to(device)\n",
        "show_imgs = 2\n",
        "random_list = np.random.choice(len(test_dataset), show_imgs, replace=False)\n",
        "\n",
        "for i in range(show_imgs):\n",
        "    idx = random_list[i]\n",
        "    sample = test_dataset[idx]\n",
        "    pred = model(sample['x'].to('cuda', dtype=torch.float32).unsqueeze(0))\n",
        "    pred = torch.sigmoid(pred).squeeze(0).squeeze(0)\n",
        "    pred = pred.data.cpu().numpy()\n",
        "    pred = np.where(pred<0.5, 0, 1).astype(np.int16)\n",
        "    pred_img = Image.fromarray(np.uint8(pred), 'L')\n",
        "\n",
        "    img_view = sample['img_view']\n",
        "    img_view = Image.fromarray(img_view, 'RGB')\n",
        "\n",
        "    mask_view = sample['mask_view']\n",
        "    mask_view = Image.fromarray(mask_view, 'L')\n",
        "\n",
        "    f, axarr = plt.subplots(1, 3)\n",
        "    axarr[0].imshow(img_view)\n",
        "    axarr[0].set_title('Input')\n",
        "    axarr[1].imshow(pred_img)\n",
        "    axarr[1].set_title('pred')\n",
        "    axarr[2].imshow(mask_view)\n",
        "    axarr[2].set_title('gt')\n",
        "    plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-22T20:24:04.445096Z",
          "iopub.execute_input": "2024-09-22T20:24:04.447358Z",
          "iopub.status.idle": "2024-09-22T20:24:07.078714Z",
          "shell.execute_reply.started": "2024-09-22T20:24:04.447317Z",
          "shell.execute_reply": "2024-09-22T20:24:07.077766Z"
        },
        "trusted": true,
        "id": "qOuglYuFSfZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GradCam visualisation\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "# Step 1: Load the model (ensure it's defined as per your architecture)\n",
        "model.load_state_dict(torch.load('best.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Step 2: Load and preprocess the input image\n",
        "def load_image(image_path):\n",
        "    # Load the image\n",
        "    image = Image.open(image_path).convert(\"RGB\")  # Ensure it's RGB\n",
        "\n",
        "    # Define the transformations\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize((512, 512)),  # Resize to match model input\n",
        "        transforms.ToTensor(),  # Convert to tensor\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
        "    ])\n",
        "\n",
        "    # Apply transformations\n",
        "    image_tensor = preprocess(image)\n",
        "    image_tensor = image_tensor.unsqueeze(0)  # Add batch dimension\n",
        "    return image_tensor\n",
        "\n",
        "# Load your input image (replace with your actual image path)\n",
        "input_image_path = 'image_path'  # Specify the path to your image\n",
        "input_tensor = load_image(input_image_path)\n",
        "input_tensor = input_tensor.to('cuda')\n",
        "\n",
        "# Step 3: Set up the hook and perform Grad-CAM\n",
        "def get_activation(layer):\n",
        "    def hook(model, input, output):\n",
        "        activation.append(output)\n",
        "    return hook\n",
        "\n",
        "# Choose the target layer for Grad-CAM\n",
        "target_layer = _layer_name_  # Modify this to your architecture\n",
        "\n",
        "# Register the hook\n",
        "activation = []\n",
        "hook = target_layer.register_forward_hook(get_activation(target_layer))\n",
        "\n",
        "# Forward pass through the model\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)\n",
        "\n",
        "# Unregister the hook\n",
        "hook.remove()\n",
        "\n",
        "# Now you can access the activation\n",
        "activation_map = activation[0]  # Get the activation from the hook\n",
        "\n",
        "# Assuming the output shape is [N, C, H, W]\n",
        "# For visualization, we need to take the average across the channels\n",
        "activation_map = activation_map.mean(dim=1, keepdim=True)  # Shape: [N, 1, H, W]\n",
        "activation_map = F.relu(activation_map)  # ReLU activation\n",
        "\n",
        "# Normalize the activation map for visualization\n",
        "activation_map = (activation_map - activation_map.min()) / (activation_map.max() - activation_map.min())\n",
        "activation_map = activation_map.squeeze().cpu().numpy()  # Convert to numpy for plotting\n",
        "\n",
        "# Plot the activation map\n",
        "plt.imshow(activation_map, cmap='inferno')  # or any other colormap\n",
        "plt.axis('off')  # Turn off axis\n",
        "#plt.colorbar()  # Show colorbar for reference\n",
        "#plt.title('Grad-CAM Activation Map')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OPyVDKrr4sc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**With the help of the Segmentation Models Pytorch GitHub repository by Pavel Iakubovskii, the code implementation for this research was partially adapted [@misc{Iakubovskii:2019}]{\\url{https://github.com/qubvel/segmentation_models.pytorch}}.**"
      ],
      "metadata": {
        "id": "yDVe0x1g5jPU"
      }
    }
  ]
}